1
00:00:50,079 --> 00:00:57,079
இன்று நாம் நமது இரண்டாவது விரிவுரை தொடங்கும்
செயற்கை நுண்ணறிவு. முதல் விரிவுரை

2
00:01:00,440 --> 00:01:07,440
நாங்கள் அதாவது செயற்கை என்ன சில விஷயங்களை அறிமுகப்படுத்தப்பட்டது
உளவுத்துறை வரையறை அனைத்து பற்றி,

3
00:01:08,049 --> 00:01:15,049
உளவுத்துறை மற்றும் நாம் பல உதாரணங்கள் பார்த்து
AI அமைப்புகளின் மற்றும் நாங்கள் வரலாற்றைக்

4
00:01:15,140 --> 00:01:22,140
AI ன். இன்று நாம் இரண்டாவது பகுதியாக சென்று
நாம் பற்றி பேச அங்கு இந்த அறிமுகம்

5
00:01:22,970 --> 00:01:25,750
நுண்ணறிவுக்.

6
00:01:25,750 --> 00:01:32,000
இந்த நிச்சயமாக ஒரு முக்கிய பகுதியாக நாம் பேசுவோம்
ஒரு அறிவார்ந்த பல்வேறு அம்சங்கள் பற்றி

7
00:01:32,000 --> 00:01:39,000
முகவர். இன்று நாம் அறிமுகப்படுத்தும் என்ன ஒரு அறிவார்ந்த
நாம் எவ்வாறு நுண்ணறிவுக் பாருங்கள்.

8
00:01:40,789 --> 00:01:47,789
இப்போது, வழிகாட்டும் நோக்கம் விவரிக்க
இன்று ?? கள் விரிவுரை நாங்கள் வரையறுக்க விரும்புகிறேன்

9
00:01:49,619 --> 00:01:56,619
நாங்கள் அர்த்தம் என்ன என ஒரு நுண்ணறிவுக்
ஒரு முகவராக உளவுத்துறை, ஒரு அறிவார்ந்த வரையறுக்க

10
00:01:59,330 --> 00:02:00,129
agent.


00:02:00,129 --> 00:02:07,129
In the last class we talked about rationality.
We will talk about what we mean by a rational

00:02:08,629 --> 00:02:15,629
agent. Then we will explain the concept of
rationality as we see or the concept of bounded

00:02:16,940 --> 00:02:23,790
rationality which we will deal with. We will
discuss the different characteristics of the

00:02:23,790 --> 00:02:30,790
environment in which the agent operates and
we will explain different agent architectures.

00:02:38,090 --> 00:02:45,090
On completing this lesson you will be able
to understand what an agent is and how an

00:02:46,760 --> 00:02:53,760
agent interacts with the environment. As we
will see in the course of this lecture the

00:02:54,340 --> 00:03:01,340
environment is an important component of the
agent design and the agent should be designed

00:03:01,350 --> 00:03:08,350
so that it can work properly in its environment.
After you have taken todays lecture you

00:03:12,360 --> 00:03:14,010
should be able to do the following:

00:03:14,010 --> 00:03:20,520
When you are given a problem situation, you
should be able to identify which are the percepts

00:03:20,520 --> 00:03:27,520
to the available to the agent or what the
agent can sense and how the agent should act

00:03:30,530 --> 00:03:37,530
to optimize its performance. We should also
look at what are the performance measures

00:03:40,730 --> 00:03:47,730
by which we should evaluate an agent to try
to see if the agent design has been successful

00:03:48,380 --> 00:03:55,380
and we hope to understand what we mean by
the definition of what a rational agent is.

00:03:56,770 --> 00:04:03,770
We will also look at the concept of bounded
rationality which is the rationality that

00:04:05,099 --> 00:04:12,099
we will deal with. In summary we will be familiar
with different agent architectures including

00:04:16,590 --> 00:04:23,590
stimulus response agents, state based agents,
deliberative or goal directed agents, utility

00:04:25,590 --> 00:04:32,590
based agents as well as learning agents.

00:04:34,860 --> 00:04:40,599
And we should also we able to identify given
a problem situation the characteristics of

00:04:40,599 --> 00:04:47,599
the environment and recommend what the architecture
of the desired agent should be in this environment.

00:04:54,190 --> 00:05:01,190
So this is our agent. The 
agent operates in an environment. The agent
receives percepts from the environment and

00:05:14,129 --> 00:05:21,129
the agent acts and its actions can change
the environment. The agent uses its various

00:05:26,499 --> 00:05:33,499
sensory organs so depending upon the sensors
that the agent has for example the agent may

00:05:33,779 --> 00:05:40,309
be able to see if the agent has a camera,
the agent may be able to hear if it has a

00:05:40,309 --> 00:05:47,309
sonar sensor and so agent can see or hear
or accept different inputs from the environment.

00:05:50,599 --> 00:05:57,599
Inside the agent there is an agent program
which decides on the basis of the current

00:05:58,110 --> 00:06:05,110
percept or the percept sequence it has received
till date to decide what should be the good

00:06:08,960 --> 00:06:15,960
action to take in the current situation. So
the agent has actuators or effectors to take

00:06:21,779 --> 00:06:28,779
actions. These actions can potentially change
the environment and the agent can use its

00:06:33,839 --> 00:06:40,839
sensors to sense the changed environment.

00:06:49,349 --> 00:06:56,349
The agent does the following things:
The agent operates in an environment. The

00:06:57,589 --> 00:07:04,589
agent perceives its environment through its
sensors. The agent acts upon the environment

00:07:06,830 --> 00:07:13,830
through actuators or effectors and also the
agent has goals.

00:07:14,080 --> 00:07:21,080
Goals are the objectives which the agent has
to satisfy and the actions that the agent

00:07:21,659 --> 00:07:28,659
will take will depend upon the goal it wants
to achieve.

00:07:31,789 --> 00:07:37,550
What is a percept?
The complete set of inputs at a given time

00:07:37,550 --> 00:07:43,919
the agent gets is called its percept. The
input can be from the keyboard or through

00:07:43,919 --> 00:07:50,919
its various sensors. The sequence of percepts
may be the current percept or may be all the

00:07:51,939 --> 00:07:58,939
percepts that the agent has perceived so far
can influence the actions of an agent. The

00:08:03,439 --> 00:08:10,439
agent can change the environment through effectors
or actuators. An operation which involves

51
00:08:16,699 --> 00:08:23,699
an actuator is called an action. So, agent
can take action in an environment through

52
00:08:24,089 --> 00:08:31,089
the output device or through the different
actuators that it might be having. These actions

53
00:08:32,550 --> 00:08:36,969
can be grouped into action sequences.

54
00:08:36,969 --> 00:08:43,969
We have already seen that the agent has sensors
and actuators and has goals. And the agent

55
00:08:48,320 --> 00:08:55,320
program implements a mapping from percept
sequences to actions.

56
00:09:14,480 --> 00:09:21,330
We also have to talk about a performance measure
by which we will evaluate an agent, evaluate

57
00:09:21,330 --> 00:09:28,330
how successful the agent design has been.
And finally we will like to talk about autonomous

58
00:09:31,160 --> 00:09:32,260
agents.

59
00:09:32,260 --> 00:09:38,440
In artificial intelligence artificial intelligent
agents autonomy is extremely important.

60
00:09:38,440 --> 00:09:45,440
The agent should be able to decide autonomously
which action it should take in the current

61
00:09:45,660 --> 00:09:52,660
situation. So an autonomous agent decides
autonomously what action to take in its current

62
00:09:56,310 --> 00:10:03,310
situation in order to maximize its progress
towards its goals. So, if the agent has a

63
00:10:03,680 --> 00:10:10,680
goal the agent should try to maximize its
goal too so that its performance measure is

64
00:10:11,010 --> 00:10:18,010
at maximum. The behavior and performance of
intelligent agents we will look at in terms

65
00:10:27,130 --> 00:10:32,140
of the agent function. An agent function as
we have already talked about is a mapping

66
00:10:32,140 --> 00:10:36,200
from perception history to action.

67
00:10:36,200 --> 00:10:42,450
Now what is the mapping that the agent should
implement?

68
00:10:42,450 --> 00:10:49,450
Obviously the mapping the agent should implement
is the one which maximizes its performance

69
00:10:50,700 --> 00:10:57,700
measure. The ideal mapping of an agent specifies
which action an agent should take at any point

70
00:11:00,530 --> 00:11:07,530
of time. We will talk presently about how
the agent should achieve this maximization.

71
00:11:11,800 --> 00:11:17,810
The performance measure is in fact a subjective
measure which characterizes how successful

72
00:11:17,810 --> 00:11:22,580
an agent is. And how the performance measure
is characterized can vary.

73
00:11:22,580 --> 00:11:29,580
The performance measure could be the amount
of how rich the agent will become if the agent

74
00:11:29,640 --> 00:11:36,640
behaves in a particular way or how quickly
the problem can be solved or how precise or

75
00:11:38,430 --> 00:11:44,300
how good the solution is, what is the quality
of the solution that the agent has been able

76
00:11:44,300 --> 00:11:51,300
to achieve, the amount of power the agents
objective may be to minimize the amount of

77
00:11:51,910 --> 00:11:57,980
power consumed or the performance measure
could be a combinations of several of these

78
00:11:57,980 --> 00:12:03,340
factors.

79
00:12:03,340 --> 00:12:10,340
Actually an agent could be anything. We can
look upon a human being as an agent, we can

80
00:12:14,370 --> 00:12:20,510
look upon a calculator as an agent. So, in
order to characterize something as an agent

81
00:12:20,510 --> 00:12:27,510
we have to look at the different characteristics
in terms of the different mappings the agent

82
00:12:28,970 --> 00:12:34,990
performs such as its percepts, its actions,
the environment it operates, the entire characteristics

83
00:12:34,990 --> 00:12:40,780
of the environment and certain other things.
So let us look at some common things that

84
00:12:40,780 --> 00:12:47,450
we are familiar with and look at what are
the characteristics associated with such agents.

85
00:12:47,450 --> 00:12:52,040
We are all familiar agents, so what are our
sensory organs?

86
00:12:52,040 --> 00:12:58,040
We can see with our eyes, we can hear with
our ears, we can smell, we can touch, we can

87
00:12:58,040 --> 00:13:05,040
taste so these are the five primary input
mechanisms we possess.

88
00:13:06,040 --> 00:13:11,490
What are the actuators we possess?
We have our hands with which we can take some

89
00:13:11,490 --> 00:13:17,330
action, we have our fingers, we have our legs
and several other things including the mouth

90
00:13:17,330 --> 00:13:21,440
with which we can take our actions.

91
00:13:21,440 --> 00:13:28,440
Now, if we design a robot what are the percepts,
what are the different sensory organs we can

92
00:13:32,510 --> 00:13:36,160
give to the robot that we can build into a
robot?

93
00:13:36,160 --> 00:13:43,160
We can have a camera with a robot, a camera
with which it can take pictures of what is

94
00:13:43,840 --> 00:13:50,100
in front of the robot and the robot function
or the robot program can analyze that image

95
00:13:50,100 --> 00:13:56,490
and find out salient characteristics of the
environment which will decide the way it should

96
00:13:56,490 --> 00:14:03,370
act. Then the robot can use other types of
sensors like sonar sensors, infra red sensors

97
00:14:03,370 --> 00:14:10,370
etc. And the type of actuators we can have
or we can build into a robot are wheels, speakers,

98
00:14:14,440 --> 00:14:21,440
lights, grippers and any other output device
that we may want to. And then we can look

99
00:14:24,780 --> 00:14:25,900
at a software agent.

100
00:14:25,900 --> 00:14:31,200
The software agent is becoming increasingly
common now-a-days. So many people call them

101
00:14:31,200 --> 00:14:38,200
softbots. These softbots have some functions
as sensors. They compute a function of the

102
00:14:41,070 --> 00:14:47,110
input. So they have some functions as sensors
and some functions as actuators. We will later

103
00:14:47,110 --> 00:14:53,500
look at some specific software agents to look
at the sort of function which they take as

104
00:14:53,500 --> 00:15:00,500
the input and the sort of function they compute
to give the output.

105
00:15:00,940 --> 00:15:07,940
This is a picture of a robot. This is the
Xavier robot developed at CMU it was one of

106
00:15:08,390 --> 00:15:13,400
the earlier robots that was developed at CMU.

107
00:15:13,400 --> 00:15:20,400
Then this is a robot called the Cog robot
which was developed by Rodney Brooks group

108
00:15:21,360 --> 00:15:26,670
at MIT.

109
00:15:26,670 --> 00:15:33,670
We will have an occasion to talk more about
Rodney Brooks activity in the course of

110
00:15:41,160 --> 00:15:48,160
todays lecture. So the basic motivation
behind creating Cog is the hypothesis that

111
00:15:49,850 --> 00:15:56,850
they wanted to build a humanoid robot a robot
having intelligence like a human being. This

112
00:15:58,550 --> 00:16:05,550
picture shows professor Rodney Brooks with
the robot Cog.

113
00:16:13,060 --> 00:16:20,060
And thirdly we have this entertainment robot.
This Aibo robot is sold by Sony. This robot

114
00:16:26,500 --> 00:16:33,500
is claimed to be autonomous, sensitive to
its environment, it can learn, it has different

115
00:16:35,730 --> 00:16:41,960
stages of its life and it can be personalized
according to the environment in which it grows

116
00:16:41,960 --> 00:16:45,920
up in.

117
00:16:45,920 --> 00:16:52,730
Then there are other types of agents like
softbots or software agents. For example,

118
00:16:52,730 --> 00:16:59,730
there are sites like askjeeves dot com which
you can consider as an example of a software

119
00:17:01,279 --> 00:17:07,760
agent. There are various expert systems including
the medical expert systems for example, the

120
00:17:07,760 --> 00:17:14,760
cardiologist which are also software agents.
Other than software agents we have other types

121
00:17:14,980 --> 00:17:21,980
of robots for example, the autonomous spacecrafts
like the Mars Rover and then there are other

122
00:17:24,029 --> 00:17:29,539
types of robots like intelligent buildings
which have intelligence built into them to

123
00:17:29,539 --> 00:17:36,539
decide the lighting condition, air conditioning
etc.

124
00:17:38,710 --> 00:17:45,710
So an intelligent agent must have certain
fundamental faculties. It should be able to

125
00:17:45,779 --> 00:17:52,779
act. An intelligent agent must act. The intelligent
agent must sense its environment. An agent

126
00:17:59,730 --> 00:18:05,480
must sense its environment. If the agent acts
without sensing, it is a blind action and

127
00:18:05,480 --> 00:18:12,480
blind action cannot be intelligent. So there
are certain types of architectures where the

128
00:18:13,110 --> 00:18:20,110
agents can sense and act and the agents do
not do, do not perform any deeper deliberative

129
00:18:22,799 --> 00:18:29,799
action. In fact robotics is about sensing
and acting only. At the outside we only require

130
00:18:32,749 --> 00:18:39,749
the agent to act appropriately. Understanding
is not necessary. Understanding may be important

131
00:18:40,269 --> 00:18:46,139
for choosing proper actions but understanding
by itself may not be necessary.

132
00:18:46,139 --> 00:18:53,139
However, we must realize that sensing really
needs understanding to be useful. So an intelligent

133
00:18:58,720 --> 00:19:05,559
agent who possesses complete intelligence
must be able to do the following:

134
00:19:05,559 --> 00:19:12,559
It must be able to understand or interpret
what it senses, it should be able to reason

135
00:19:13,299 --> 00:19:20,299
and finally the agent should also be able
to learn so that the agent can operate in

136
00:19:20,600 --> 00:19:27,600
an unknown environment. In fact learning is
a prerequisite for the agent to be autonomous.

137
00:19:29,429 --> 00:19:36,429
An autonomous agent which can adjust to a
changing environment must have some learning

138
00:19:37,710 --> 00:19:40,789
component.

139
00:19:40,789 --> 00:19:47,789
Also, intelligent agents must be rational.

140
00:19:53,850 --> 00:20:00,850
In artificial intelligence we will talk about
building rational agents and we will talk

141
00:20:01,850 --> 00:20:08,799
about different aspects the rational agent
has and look at the .

142
00:20:08,799 --> 00:20:15,799
Now, what is a rational agent?
A rational agent is an agent which always

143
00:20:16,820 --> 00:20:18,200
does the right thing.

144
00:20:18,200 --> 00:20:23,990
Now what is the right thing?
So in order to understand that we must understand

145
00:20:23,990 --> 00:20:30,840
what are the functionalities or the goals
of the agent. What are the components of the

146
00:20:30,840 --> 00:20:37,840
agent and we must look at how we should build
these agents.

147
00:20:38,080 --> 00:20:45,080
Perfect rationality assumes that the rational
agent knows everything and will take that

148
00:20:49,399 --> 00:20:56,399
action which maximizes her utility. So, perfect
rationality is prevalent to demanding that

149
00:20:58,869 --> 00:21:05,869
the agent is omniscient or all knowing. If
the agent knows everything and the agent can

150
00:21:06,489 --> 00:21:13,489
reason extremely fast then the agent is a
perfect rational agent. However, a perfect

151
00:21:17,629 --> 00:21:24,629
rationality is something which is not within
the scope of even human beings. We do not

152
00:21:26,559 --> 00:21:32,070
satisfy the definition of perfect rationality.
We are not omniscient; there are many things

153
00:21:32,070 --> 00:21:39,070
which we do not know, there are many things
which we cannot reason in a reasonable time

154
00:21:39,309 --> 00:21:46,309
frame given what we know. The concept of bounded
rationality was introduced by Herbert Simon

155
00:21:50,309 --> 00:21:57,309
of CMU in 1972 in his theory of Economics.
So, bounded rationality says that because

156
00:21:59,230 --> 00:22:06,230
of the limitations of the human mind humans
must use approximate methods to handle many

157
00:22:07,019 --> 00:22:07,970
tasks.

158
00:22:07,970 --> 00:22:14,970
Hence, bounded rationality does not aim at
maximizing the absolute utility of the agent

159
00:22:17,799 --> 00:22:24,799
because that is something which may not be
achievable given a realistic agent given realistic

160
00:22:26,889 --> 00:22:33,889
resource path. So, instead rationality as
we will look at we will concentrate on using

161
00:22:38,509 --> 00:22:45,509
approximate methods where appropriate so that
the agent can take the best action given its

162
00:22:47,590 --> 00:22:54,590
resource limitation and given what it already
knows.

163
00:22:57,799 --> 00:23:04,799
Rational action is the action that maximizes
the expected value of the performance given

164
00:23:07,899 --> 00:23:14,899
the percept sequence to date. When an agent
takes an action the action may not always

165
00:23:16,259 --> 00:23:21,309
have the same outcome. Sometimes the outcome
of the action may be good, sometimes it may

166
00:23:21,309 --> 00:23:27,940
be bad. So, when we talk about rationality
we must look at the expected value of the

167
00:23:27,940 --> 00:23:34,570
performance measure and not always the absolute
value of the performance measure.

168
00:23:34,570 --> 00:23:41,570
Also, we must evaluate an agent not on the
basis of how it should behave in a perfect

169
00:23:42,279 --> 00:23:49,279
case but how it can behave based on what it
can sense. So something that the agent cannot

170
00:23:53,210 --> 00:24:00,210
sense or foresee we cannot hold the agent
responsible for those things. Therefore rational

171
00:24:02,489 --> 00:24:09,049
action in the light of bounded rationality
talks about maximizing the expected value

172
00:24:09,049 --> 00:24:16,049
of the performance measure given the percept
sequence which is available to the agent.

173
00:24:16,389 --> 00:24:23,389
Therefore does rational action means the best
action?

174
00:24:24,399 --> 00:24:31,399
The answer is yes but to the best of the agents
knowledge based on what percepts the agent

175
00:24:33,299 --> 00:24:35,419
already has access to.

176
00:24:35,419 --> 00:24:42,299
Does rational mean optimum?
Yes, but to the best of the abilities of the

177
00:24:42,299 --> 00:24:49,299
agent and subject to the resource constraints.
The agent may have limitations in the way

178
00:24:50,489 --> 00:24:56,350
it can act. The agent may have limitations
in terms of the resources that it has access

179
00:24:56,350 --> 00:25:02,629
to. There could be time limitations, there
could be space limitations. So, given these

180
00:25:02,629 --> 00:25:09,629
constraints given its abilities the agent
should take the best action that is expected

181
00:25:10,289 --> 00:25:16,830
to maximize its utility. That is the rationality
we will talk about.

182
00:25:16,830 --> 00:25:23,830
We must understand that a rational agent need
not be omniscient because it does not know

183
00:25:25,489 --> 00:25:32,149
the actual outcome of its actions. And certain
aspects of the environment may be unknown

184
00:25:32,149 --> 00:25:39,149
to the agent. So rationality takes into account
the limitations of the agent, the percept

185
00:25:40,090 --> 00:25:45,320
sequence that it has access to, the background
knowledge the agent has and the actions that

186
00:25:45,320 --> 00:25:52,320
the agent can take. And we only deal with
the expected outcome of actions.

187
00:25:55,299 --> 00:26:02,299
As we have already talked about, in 1957 Herbert
Simon proposed the notion of bounded rationality.

188
00:26:03,379 --> 00:26:09,840
Formally if we define bounded rationality,
It is that property of an agent that behaves

189
00:26:09,840 --> 00:26:16,840
in a manner that is nearly optimal with respect
to its goal as its resources will allow as

190
00:26:17,059 --> 00:26:24,059
nearly optimal as its resources will allow.

191
00:26:25,909 --> 00:26:32,909
Now we come to another very important component
of an agent. Actually the design of the agent

192
00:26:33,830 --> 00:26:40,830
depends a lot on the environment in which
the agent operates. And the task environment

193
00:26:41,279 --> 00:26:48,279
of the agent can be defined in different ways.
So we will look at several ways of characterizing

194
00:26:50,919 --> 00:26:54,989
the environment. There are two ways of characterizing
an environment. One way of characterizing

195
00:26:54,989 --> 00:27:01,989
the environment is in the absolute sense and
another is from the point of view of the agent.

196
00:27:02,869 --> 00:27:09,869
For example, when we want to talk about whether
an environment is deterministic or stochastic

197
00:27:11,869 --> 00:27:18,869
we must not look at whether the environment
is deterministic from an absolute point of

198
00:27:20,320 --> 00:27:26,359
view but rather whether the agent appears,
whether the environment appears deterministic

199
00:27:26,359 --> 00:27:31,909
to the agent based on what it can perceive.

200
00:27:31,909 --> 00:27:38,909
Environments can be divided into two types
based on the observability of the environment.

201
00:27:47,600 --> 00:27:54,600
An environment may be fully observable or
it may be partially observable. If the environment

202
00:27:56,609 --> 00:28:03,609
is fully observable the entire environment
which is relevant to the action being considered

203
00:28:05,330 --> 00:28:12,330
is observable. All the relevant portions of
the environment are observable.

204
00:28:15,409 --> 00:28:22,409
On the other hand, in the case of partially
observable environments not all the relevant

205
00:28:22,479 --> 00:28:29,479
portions may be observed. The relevant features
of the environment are only partially observable.

206
00:28:31,129 --> 00:28:38,119
For example, consider an agent playing chess
like the deep blue chess playing program.

207
00:28:38,119 --> 00:28:45,119
The agent has complete knowledge of the board.
So everything about the environment is accessible

208
00:28:47,619 --> 00:28:53,720
to the agent. So the chess environment is
a fully observable environment.

209
00:28:53,720 --> 00:29:00,649
Consider again the environment where the agent
is playing pokers where the agent cannot see

210
00:29:00,649 --> 00:29:07,649
the hand of the opponent. So the environment
in this case is only partially observable

211
00:29:07,970 --> 00:29:14,970
to the agent. So, if the environment is fully
observable the agent need not keep track of

212
00:29:16,440 --> 00:29:23,440
how the environment is changing and deliberating
about the environment. So, if the environment

213
00:29:24,859 --> 00:29:31,090
is partially observable the agent in order
to behave successfully in such an environment

214
00:29:31,090 --> 00:29:37,460
has an added task of keeping track of the
environment and reasoning about the properties

215
00:29:37,460 --> 00:29:41,559
of the environment.

216
00:29:41,559 --> 00:29:48,559
Then, if you look at the aspect of determinism
again environments can be divided into two

217
00:29:50,940 --> 00:29:55,200
or three types.

218
00:29:55,200 --> 00:30:01,149
Deterministic environments: In deterministic
environments the next state of the environment

219
00:30:01,149 --> 00:30:08,029
is completely described by the current state
and the agents action. When we looked at

220
00:30:08,029 --> 00:30:15,029
diagram of agent environment we have already
seen that the agents action changes the

221
00:30:17,519 --> 00:30:24,519
environment. Now, in a deterministic environment
given the agents action how the environment

222
00:30:25,029 --> 00:30:32,029
change is deterministic. There is no other
unknown thing which comes into the picture

223
00:30:34,460 --> 00:30:39,149
to decide how the environment changes.

224
00:30:39,149 --> 00:30:46,149
In stochastic environments, on the other hand,
there is some element of uncertainty. Therefore

225
00:30:50,590 --> 00:30:57,590
how the environment changes depends not just
on the action that the agent takes. So, whether

226
00:31:02,049 --> 00:31:09,049
an environment is deterministic or stochastic
depends on how you look at the environment.

227
00:31:11,950 --> 00:31:18,950
If you cannot observe the environment fully
the environment may appear stochastic to you

228
00:31:19,379 --> 00:31:26,379
whereas it is actually deterministic if you
have access to the entire environment.

229
00:31:26,559 --> 00:31:33,559
For example, take the example of pokers again.
If the agent did have access to both the players

230
00:31:35,359 --> 00:31:42,359
hands and all the cards which are there then
the environment actually behaves in a deterministic

231
00:31:43,289 --> 00:31:50,289
way. However when it is partially observable
it appears to be a stochastic environment.

232
00:31:51,869 --> 00:31:57,609
So this is an environment which appears to
be stochastic because of partial observability.

233
00:31:57,609 --> 00:32:04,609
If you look at the game of Ludo it is a stochastic
environment because how the environment evolves

234
00:32:08,519 --> 00:32:15,519
depend on the value which the die has rolled.
So whether the die has rolled 1 or 2 or 6

235
00:32:17,700 --> 00:32:22,080
depending on that the agent can decide its
actions. Therefore this is a truly stochastic

236
00:32:22,080 --> 00:32:23,629
environment.

237
00:32:23,629 --> 00:32:30,629
A strategic environment is an environment
state which is wholly determined by the preceding

238
00:32:31,769 --> 00:32:38,769
state and the actions of multiple agents.
In a strategic environment apart from the

239
00:32:42,039 --> 00:32:49,039
current agent itself there are other agents
around and the actions of all these agents

240
00:32:49,109 --> 00:32:55,739
influence the environment. A strategic environment
is one where the environment is only changed

241
00:32:55,739 --> 00:33:02,739
by the actions of the agent itself and other
agents. For example, a chess playing agent

242
00:33:05,929 --> 00:33:12,929
operates in a strategic environment.

243
00:33:19,200 --> 00:33:26,200
Then if we look at the episodicity of the
environment an environment may be episodic

244
00:33:26,759 --> 00:33:33,759
or sequential. An episodic environment is
one where subsequent episodes do not depend

245
00:33:37,409 --> 00:33:44,409
on what actions occurred in previous episodes.
First of all sometimes certain tasks can be

246
00:33:44,440 --> 00:33:51,440
divided into different phases or different
episodes. In such episodic environments the

247
00:33:54,470 --> 00:34:01,470
one episode may or may not influence the subsequent
episode. So, if one episode does not influence

248
00:34:03,499 --> 00:34:08,440
the subsequent episode such an environment
is called an episodic environment.

249
00:34:08,440 --> 00:34:15,440
Therefore in such an environment an agent
did not plan beyond episodes. In a sequential

250
00:34:16,960 --> 00:34:23,960
environment, on the other hand, the agent
engages in a series of connected episodes.

251
00:34:24,330 --> 00:34:31,330
So subsequent episodes are dependent on what
happened in the previous episodes. Hence,

252
00:34:32,970 --> 00:34:39,970
in this case the agent may need to plan ahead
or base its action upon what happened in the

253
00:34:40,590 --> 00:34:47,590
previous episodes.

254
00:34:48,710 --> 00:34:55,710
Then if you look at another characteristic
of the environment is its dynamism. Environments

255
00:34:56,800 --> 00:35:03,800
may be static or dynamic. A static environment
does not change by itself. So an advantage

256
00:35:06,250 --> 00:35:13,250
is that a static environment does not change
when the agent is deliberating. The agent

257
00:35:15,480 --> 00:35:22,480
does not need to observe the world during
its thinking process. A dynamic environment

258
00:35:23,070 --> 00:35:30,070
changes by itself that is apart from the action
that the agent takes.

259
00:35:32,430 --> 00:35:39,430
Also, another dimension by which we can characterize
environments is by its continuity. An environment

260
00:35:45,330 --> 00:35:52,330
is discrete or continuous. An environment
is discrete if the number of distinct percepts

261
00:35:53,240 --> 00:36:00,240
and actions are limited and the number of
states is limited. If the number of states

262
00:36:00,470 --> 00:36:05,510
is discrete then that environment is a discrete
environment. And the environment is a continuous

263
00:36:05,510 --> 00:36:12,510
environment if the range of percepts is continuous
or the range of actions that it can take is

264
00:36:12,910 --> 00:36:19,910
many or the number of states is either continuous
or too many so that we treat them as continuous.

265
00:36:26,030 --> 00:36:33,030
And finally we can also characterize the environment
according to whether there is one agent whom

266
00:36:34,130 --> 00:36:40,860
we are talking about in the environment or
whether there are other multiple regions in

267
00:36:40,860 --> 00:36:47,860
the environment. That is, if the environment
contains other agents then it is a multi agent

268
00:36:50,360 --> 00:36:57,360
environment. In games when we look at two
person games we usually talk about one opponent

269
00:36:59,810 --> 00:37:06,810
agent. There are environments where there
is a single agent but for many tasks that

270
00:37:11,180 --> 00:37:18,180
the agent does usually we talk about a single
agent environment but there are many situations

271
00:37:19,210 --> 00:37:26,210
which require distributed agents or with social
and economic systems we deal with multiple

272
00:37:28,290 --> 00:37:33,480
agent systems.

273
00:37:33,480 --> 00:37:40,480
Therefore the complexity of the environment
includes knowledge rich environments and input

274
00:37:43,660 --> 00:37:50,450
rich environments. In knowledge rich environments
there is enormous amount of information the

275
00:37:50,450 --> 00:37:57,450
environment contains. Knowledge rich environments
contain lots of information. And input rich

276
00:37:58,280 --> 00:38:04,450
environment is one where there is enormous
amount of input, enormous amount of percept

277
00:38:04,450 --> 00:38:11,450
that the agent can get. And in such environments
which are complex, which are either knowledge

278
00:38:12,280 --> 00:38:19,280
rich or which are percept rich the agent must
have a way of managing this complexity. So,

279
00:38:21,680 --> 00:38:28,680
such considerations in the environment are
complex. The agent must develop it strategies

280
00:38:29,730 --> 00:38:36,730
for sensing or its strategies of attentional
mechanisms. The agent must decide selectively

281
00:38:38,310 --> 00:38:45,250
what to sense, what is more relevant and what
it should give its attention to rather than

282
00:38:45,250 --> 00:38:52,250
deal with the entire complexity of the full
environment. Thus, an agent needs to focus

283
00:38:53,500 --> 00:38:59,130
its effort in such rich environments.

284
00:38:59,130 --> 00:39:06,130
Now we will look at the different types of
agent architectures. Table based agents, stimulus

285
00:39:09,040 --> 00:39:15,370
response agents, goal based agents, and utility
based agents and learning agents. We will

286
00:39:15,370 --> 00:39:22,370
briefly talk about these agent architectures.
A table base agent is a very simple agent.

287
00:39:28,030 --> 00:39:35,030
An agent has to take actions given its sense
given what it senses, given its percepts.

288
00:39:38,480 --> 00:39:45,480
Now, in a table based agent the mapping from
percepts to actions is stored in the form

289
00:39:47,550 --> 00:39:54,550
of a table. Therefore, on the left side we
have the percepts and on the right side we

290
00:39:56,570 --> 00:40:03,570
have actions. And the agent program is extremely
simple. Hence, given the percept the agent

291
00:40:07,950 --> 00:40:14,950
looks at the table and decides what action
to take. Therefore based on this mechanism

292
00:40:15,000 --> 00:40:22,000
we can develop what we call reactive agents
which can take the action depending on the

293
00:40:22,630 --> 00:40:29,630
percept. Unfortunately the best action for
the agent it depends not only on the current

294
00:40:31,230 --> 00:40:38,230
percept but on the percept sequence.

295
00:40:39,120 --> 00:40:45,010
Now the number of possible percept sequence
can be very large. In fact it can be infinite

296
00:40:45,010 --> 00:40:52,010
till the agent acts over many time steps,
many unbounded time steps then it can become

297
00:40:52,530 --> 00:40:58,480
infinite. So this table can become very large
if you have a mapping from percept sequence

298
00:40:58,480 --> 00:41:05,480
to action. In simple tasks where the action
only depends on the current percept it may

299
00:41:07,890 --> 00:41:14,890
be easy to develop a table based agent but
it is infeasible when the correct action or

300
00:41:17,820 --> 00:41:24,750
the best action depends on the past and not
only what the agent sees or perceives at the

301
00:41:24,750 --> 00:41:31,080
current time step.

302
00:41:31,080 --> 00:41:38,080
So, a table is a very simple way to specify
a mapping from percepts to actions but tables

303
00:41:43,110 --> 00:41:50,110
may become very large. And in a table based
agent there is no intelligence in the agent

304
00:41:50,270 --> 00:41:56,780
itself and the entire work is done by the
designer in designing the table. So the designer

305
00:41:56,780 --> 00:42:03,780
makes the table and the agent just looks it
up to decide how to act. So such agents have

306
00:42:05,020 --> 00:42:12,020
no autonomy. All actions, all behaviors are
predetermined and there is really no concept

307
00:42:13,790 --> 00:42:15,310
of learning.

308
00:42:15,310 --> 00:42:22,310
Now, the mapping from percept to action may
be done in different ways. It may be in terms

309
00:42:23,110 --> 00:42:28,540
of a natural table, it may be in terms of
a production system or rules. This mapping

310
00:42:28,540 --> 00:42:35,540
can also be implemented by neural networks
or one can implement the mapping by algorithm.

311
00:42:35,890 --> 00:42:42,780
So, this mapping can be implemented in a rule
based manner using a neural network or using

312
00:42:42,780 --> 00:42:49,780
some algorithms.

313
00:42:52,090 --> 00:42:59,090
So such agents where the action depends only
on the percepts and there is no deliberation

314
00:43:00,290 --> 00:43:07,290
involved are called reactive agents. The term
reactive agent means those agents whose information

315
00:43:12,400 --> 00:43:19,400
come from sensors and they can take actions
through their actuators or effectors. And

316
00:43:21,070 --> 00:43:28,070
we can additionally assume that the actions
change the current state of the world and

317
00:43:31,950 --> 00:43:38,950
the agent can only take some action without
any deliberation. Such agents are also called

318
00:43:39,160 --> 00:43:46,050
stimulus response agents. They have no notion
of history but all their history is encoded

319
00:43:46,050 --> 00:43:53,050
in the current state as the sensors see it
right now.

320
00:43:56,480 --> 00:44:03,480
Now let us talk about professor Rodney Brooks
of MIT and the subsumption architecture he

321
00:44:05,730 --> 00:44:12,730
proposed based on stimulus response agents.
In 1986 professor Rodney Brooks gave this

322
00:44:15,970 --> 00:44:22,970
notion of the subsumption architecture. His
argument was that lower animals behave in

323
00:44:24,710 --> 00:44:31,710
a largely reactive manner. They have a very
little sense of deliberation. Therefore, most

324
00:44:35,230 --> 00:44:42,230
of their actions are reactive actions. And
his argument was that in the time scale of

325
00:44:43,970 --> 00:44:50,970
evolution reactive behavior came much earlier
than deliberative behavior. And he has been

326
00:44:54,230 --> 00:45:01,230
able to show that a number of such components
having simple reactive behavior can achieve

327
00:45:04,280 --> 00:45:08,960
a surprising degree of intelligence.

328
00:45:08,960 --> 00:45:15,960
Thus, Brooks idea has been to follow the
evolutionary path how life has evolved and

329
00:45:16,550 --> 00:45:22,790
build simple agents for complex worlds. It
is a combination of simple agents working

330
00:45:22,790 --> 00:45:29,790
well in the complex world. The features of
the subsumption architecture are that there

331
00:45:30,430 --> 00:45:37,430
is no explicit knowledge representation. Behavior
is not centrally controlled but behavior is

332
00:45:38,790 --> 00:45:45,790
distributed among different components. The
response to stimulus is reflexive and more

333
00:45:48,300 --> 00:45:55,300
importantly the design is bottom up. The complex
behaviors are fashioned from the combinations

334
00:45:55,680 --> 00:46:02,680
of simple behaviors. Simple behaviors are
put together to achieve complex behaviors.

335
00:46:05,040 --> 00:46:12,040
And each individual agent is simple and inexpensive.
Therefore the subsumption architecture is

336
00:46:16,500 --> 00:46:23,500
basically a layered architecture.

337
00:46:25,750 --> 00:46:32,750
According to professor Brooks the time scale
for evolution has been 5bn years. It has take

338
00:46:32,910 --> 00:46:39,910
5bn years to evolve from single cells since
cells evolved to the present day. The first

339
00:46:40,760 --> 00:46:47,760
humans appeared two and a half million years
ago only whereas the first cells appeared

340
00:46:48,970 --> 00:46:55,970
5bn years ago and symbols did not appear until
only 5000 years ago. Therefore his proposition

341
00:46:59,950 --> 00:47:06,950
is that we should look at simpler behavior
as the first step towards building intelligent

342
00:47:09,340 --> 00:47:10,020
agents.

343
00:47:10,020 --> 00:47:15,740
Hence, in subsumption architecture there are
different layers of behavior and higher layers

344
00:47:15,740 --> 00:47:22,740
override lower layers and each activity in
each layer consists of a simple finite step

345
00:47:22,820 --> 00:47:29,820
machine. Now, this is an example of a simple
architecture proposed by Rodney Brooks. In

346
00:47:36,490 --> 00:47:43,490
this system there are several layers. This
is a very simple robot. In the layer zero

347
00:47:45,120 --> 00:47:52,120
there is the avoid obstacles layer. In this
layer there are several sensory organs. The

348
00:47:55,860 --> 00:48:02,860
sonar component generates sonar scan, the
collide component sends halt message to forward

349
00:48:06,390 --> 00:48:13,390
and the feel force component is the signal
sent to runaway or turn around. So layer zero

350
00:48:15,400 --> 00:48:22,400
of the agent is to avoid obstacles. This is
the first layer of the agent which enables

351
00:48:22,940 --> 00:48:29,940
the agent to navigate in an environment without
colliding with other agents. So it is a very

352
00:48:30,980 --> 00:48:37,980
simple behavior of a navigating robot without
using a lot of deliberation.

353
00:48:39,120 --> 00:48:46,120
In the second layer, layer 1 there is the
wander behavior. This wander behavior generates

354
00:48:47,890 --> 00:48:54,890
a random heading. It allows the robot to occasionally
wander. So in the wander layer there are two

355
00:48:59,360 --> 00:49:06,360
components. One generates a random heading
and secondly the avoid component reads repulsive

356
00:49:07,040 --> 00:49:14,040
force and generates new heading and feeds
that to turn and to forward.

357
00:49:14,660 --> 00:49:21,660
Layer 2 that is the third layer has the exploration
behavior. So this has several components.

358
00:49:27,310 --> 00:49:34,200
The whenlook component notices whenever there
is idle time it looks for an interesting place.

359
00:49:34,200 --> 00:49:41,200
The path plan component sends new direction
to avoid. The integrate component monitors

360
00:49:44,930 --> 00:49:51,210
path and sends them to the path plan.

361
00:49:51,210 --> 00:49:58,210
So as we have seen this percept based agents
are efficient because they do not require

362
00:50:02,950 --> 00:50:08,470
internal representation for reasoning or inference.
However, there is no strategic planning or

363
00:50:08,470 --> 00:50:13,400
learning and they are not good for multiple
opposing goals.

364
00:50:13,400 --> 00:50:20,400
In the second type of agent the more knowledge
rich agents we come to state based agents.

365
00:50:21,090 --> 00:50:28,090
In state based agents information comes from
sensors and it changes internally the agents

366
00:50:31,880 --> 00:50:38,880
current view of the world based on the state
of the world and the knowledge the agent has.

367
00:50:40,110 --> 00:50:47,110
It triggers action through the effectors.
And in order to do this the agent does some

368
00:50:48,550 --> 00:50:52,160
deliberation.

369
00:50:52,160 --> 00:50:58,650
So based on the state of the world and knowledge
the agent chooses the action and carries out

370
00:50:58,650 --> 00:51:01,380
the action through the effectors.

371
00:51:01,380 --> 00:51:08,380
Now, in goal based agent the agents action
depends upon its goals and goal formulation

372
00:51:09,340 --> 00:51:16,270
is based on the current situation. Therefore,
based on the goal that the agent has to achieve

373
00:51:16,270 --> 00:51:21,820
and the current state of the agent as it perceives
the agent goes through some deliberations

374
00:51:21,820 --> 00:51:25,880
to decide what the next action should be.

375
00:51:25,880 --> 00:51:32,880
We will look at search and planning. These
are the two fields of AI in which we will

376
00:51:35,030 --> 00:51:42,030
look at different ways of deciding what action
to take in order to achieve the agents

377
00:51:44,550 --> 00:51:51,310
goals. Hence, in goal based agents the sequence
of steps required to solve a problem is not

378
00:51:51,310 --> 00:51:58,310
known a priori. They are not available in
a table and they must be determined by a systematic

379
00:51:58,460 --> 00:52:05,460
exploration of the alternative actions by
deliberations.

380
00:52:05,740 --> 00:52:12,740
Thirdly, we have utility based agents. Utility
based agents are akin to goal based agent

381
00:52:13,670 --> 00:52:19,690
but they are a more general frame work. If
an agent has several goals and it can achieve

382
00:52:19,690 --> 00:52:26,690
only some of them these goals may have preferences
associated with them as to which goals are

383
00:52:27,600 --> 00:52:34,140
more preferable. In this case we can talk
about utility based agents where we have different

384
00:52:34,140 --> 00:52:37,590
preferences for different goals.

385
00:52:37,590 --> 00:52:44,120
A utility function is a very general function
that maps a state or a sequence of states

386
00:52:44,120 --> 00:52:50,850
to a real valued utility. So a state is mapped
to a utility value and we can say that the

387
00:52:50,850 --> 00:52:57,850
goal of the agent is to maximize its utility
or the goal of the agent is to maximize its

388
00:52:57,920 --> 00:53:04,920
expected utility. When we look at the decision
theory we can consider agents which are based

389
00:53:05,030 --> 00:53:12,030
on utility. And finally and very importantly
we have learning agents.

390
00:53:13,380 --> 00:53:20,180
We have said that learning is an extremely
important component of autonomy. An autonomous

391
00:53:20,180 --> 00:53:27,180
agent should have some learning built into
it. Learning allows an agent to operate in

392
00:53:28,280 --> 00:53:34,490
unknown environments. And the learning element
modifies the performance element and learning

393
00:53:34,490 --> 00:53:40,320
is required for true autonomy. We will take
up learning later in this course when we look

394
00:53:40,320 --> 00:53:47,320
at several types of learning that can be carried
on by the agents.

395
00:53:51,960 --> 00:53:58,930
In summary of todays lecture an agent perceives
and acts in an environment, it has an architecture

396
00:53:58,930 --> 00:54:05,630
and an agent is implemented by an agent program.
An ideal agent chooses the action which maximizes

397
00:54:05,630 --> 00:54:12,630
its expected performance. An autonomous agent
uses its own experience rather than built-in

398
00:54:13,330 --> 00:54:17,740
knowledge of the environment by the designer.

399
00:54:17,740 --> 00:54:24,740
An agent program maps from percept to action
and updates its internal state. Reflex agents

400
00:54:25,540 --> 00:54:32,540
respond immediately to percepts. Goal based
agents act in order to achieve their goals.

401
00:54:32,940 --> 00:54:39,940
Utility based agents maximize their own utility
function. And then in order to have goal based

402
00:54:42,080 --> 00:54:47,480
agents the agent must represent its knowledge,
must represent its history, must have some

403
00:54:47,480 --> 00:54:54,480
background knowledge etc. So, representing
knowledge is important for successful agent

404
00:54:55,900 --> 00:54:56,630
design.

405
00:54:56,630 --> 00:55:03,630
Now we will come to a set of questions based
on todays lecture.

406
00:55:05,870 --> 00:55:12,870
Question 1: Define an agent.
Question 2: What is a rational agent?

407
00:55:15,020 --> 00:55:22,020
Question 3: What is bounded rationality?
Question 4: What is an autonomous agent?

408
00:55:24,480 --> 00:55:29,660
Question 5: Describe the salient features
of an agent.

409
00:55:29,660 --> 00:55:36,660
Question 6: Find out about the mars rover.
What are the percepts for this agent?

410
00:55:38,850 --> 00:55:42,770
Characterize the operating environment for
this agent.

411
00:55:42,770 --> 00:55:49,770
What are the actions that this agent can take?
How can one evaluate the performance of this

412
00:55:50,300 --> 00:55:54,300
agent?
What sort of agent architecture do you think

413
00:55:54,300 --> 00:56:01,300
is most suitable for this agent?
Question 7: Answer the same questions above

414
00:56:02,680 --> 00:56:09,680
for an internet shopping agent.

415
